<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PyTorch入门-Tensor综述</title>
      <link href="/article/2751853729.html"/>
      <url>/article/2751853729.html</url>
      
        <content type="html"><![CDATA[<h2 id="Tensor的理解">Tensor的理解</h2><p>数学中有标量、向量和矩阵的概念，它们的维度分别是0、1、2。其中：</p><ul><li><p><strong>标量</strong>可以看成的一个数字，<strong><code>1</code></strong>，标量中元素的位置固定。</p></li><li><p><strong>向量</strong>可以看成是一维表格，向量中元素的位置需要通过其索引确定，表示为</p><p><img src="/article/2751853729/image-20240103024202117.png" alt=""></p></li><li><p><strong>矩阵</strong>可以看成是二维表格，矩阵中的元素位置需要通过其行号和列号确定，表示为</p><p><img src="/article/2751853729/image-20240103024033803.png" alt=""></p></li></ul><p><strong>张量(Tensor) 可以视为矩阵的扩展，可以用于表示无穷维度的数据</strong></p><p>如果我们用标量、向量或矩阵描述一个事物时，该事物最多可用 [H,W]的维度表示。在现实与客观世界中，我们经常会碰到的物体的维度可能会更高维度，很难通过向量或矩阵来描述，这时我们就需要<strong>张量</strong>。也就是说，我们可以通过张量来描述任意维度的物体 <code>H * W * C</code>，其中C为C维的特征图（特征图是深度学习中的一个概念），除外我们还可以用<strong>Tensor</strong>描述更高维度的物体：<code>H * W * C* D</code>，其中D为未知的更高维空间。</p><p>总之，张量是对于标量、向量、矩阵之上进行更加泛化的定义。标量可以看成是0阶的张量，向量是1阶段张量，矩阵是2阶的张量，除了0,1,2阶的张量之外，还有<code>3阶、4阶、5阶..n阶</code>的张量。引申到<strong>深度学习</strong>，其 数据输入的维度是不确定的（可以是任意维度），这时就需要采用一个更加广泛的概念去描述这些量，Tensor就可以更便利解决该问题的。</p><h2 id="Tensor的基本概念">Tensor的基本概念</h2><p><img src="/article/2751853729/image-20240103024323595.png" alt=""></p><p>其中标量是0维的张量，向量是1维的张量，矩阵是2维的张量。</p><p>举例：如下所示，左侧为一个长方体某一切面的矩阵（二阶张量），该矩阵包含N*M个元素，其中每一个元素是一个标量，每一张都是一个向量。一个物体有N个切面，将C 个 N * M 拼接到一起，就会得到一个三阶的张量<code>C*N*M</code>用于表示长方体(如下右图)。</p><p><img src="/article/2751853729/image-20240103024343658.png" alt=""></p><blockquote><p>在用张量描述物体时，我们需要确定这个张量具体是一个什么样的量，用变量或常量来描述。</p></blockquote><h2 id="Tensor与机器学习的关系">Tensor与机器学习的关系</h2><p>完整的机器学习的任务，会涉及到样本、模型等元素（如下所示）。</p><p><img src="/article/2751853729/image-20240103024403236.png" alt=""></p><p>对于<strong>样本</strong>（机器学习中用到的数据）我们就可通过Tensor来对其进行描述。<strong>比如一条语音数据</strong>，我们可有能会采用向量来进行描述，该向量就是1阶的张量。此时向量描述的是语音数据被采样后在当前时刻的声音特征，图形化之后可能为波行。而<strong>对于灰度图</strong>，我们通常采用矩阵描述（二阶Tensor），表示成<code>H*W</code>，<strong>彩色图</strong>则会描述成<code>[H * W * C]</code>，为一个三阶Tensor，其中C=3。</p><p>而<strong>模型</strong>（模型分为<strong>有参数模型</strong>与<strong>无参数模型</strong>)，有参数模型一般被描述为  <code>Y = WX + b</code>函数，其中X是指样本，W,B是指参数。当W与B未知的情况下为变量，该变量也是通过Tensor来表示，Y为最后的标签，而标签在进行数字化时也会通过Tensor来对其进行描述。</p><blockquote><p>当样本标签与属性关系描述为<code>y=f(x)</code>，其中x为属性(样本)，f为模型。</p></blockquote><p>结论：<strong>Tensor可以用来描述机器学习过程中的样本或模型</strong>。</p><h2 id="Tensor的操作">Tensor的操作</h2><p><img src="/article/2751853729/image-20240102201942723.png" alt=""></p><ul><li>类型：Tensor类型</li><li>创建：如何创建Tensor</li><li>属性：Tensor的属性</li><li>运算：Tensor的算术运算</li><li>操作：切片、索引、变型等</li><li>与<code>numpy</code>互转：Tensor可以与numpy互转</li></ul><h2 id="Tensor类型">Tensor类型</h2><p>张量（Tensor）是Pytorch库中的基本数据类型，在 Pytorch中各种基本数字类型都有其对应的Tensor类型（但在Pytorch中没有内嵌的字符串类型）。</p><h3 id="类型列表"><strong>类型列表</strong></h3><p>Tensor的每种类型分别有对应CPU和GPU版本。加粗部分为常用类型。Tensor默认的数据类型<code>FloatTensor</code>。</p><p>Tensor类型常见的操作见 **<code>Tensor的操作</code>**章节</p><table><thead><tr><th>-</th><th>数据类型</th><th>torch</th><th>CPU Tensor</th><th>GPU Tensor</th></tr></thead><tbody><tr><td><strong>32-bit float point</strong></td><td><strong>32 bit 浮点</strong></td><td><strong>torch.float32 or torch.float</strong></td><td><strong>torch.FloatTensor</strong></td><td><strong>torch.cuda.FloatTensor</strong></td></tr><tr><td><strong>64-bit float point</strong></td><td><strong>64 bit 浮点</strong></td><td><strong>torch.float64 or torch.double</strong></td><td><strong>torch.DoubleTensor</strong></td><td><strong>torch.cuda.DoubleTensor</strong></td></tr><tr><td>16-bit float point</td><td>16 bit 半精度浮点</td><td>torch.float16 or torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td><strong>8-bit integer(unsigned)</strong></td><td>8 bit 无符号整形(0~255)****</td><td><strong>torch.uint8</strong></td><td><strong>torch.ByteTensor</strong></td><td><strong>torch.cuda.ByteTensor</strong></td></tr><tr><td>8-bit integer(signed)</td><td>8 bit 有符号整形(-128~127)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer(signed)</td><td>16 bit 有符号整形</td><td>torch.int16 or torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td><strong>32-bit integer(signed)</strong></td><td><strong>32 bit 有符号整形</strong></td><td><strong>torch.int32 or torch.int</strong></td><td><strong>torch.IntTensor</strong></td><td><strong>torch.cuda.IntTensor</strong></td></tr><tr><td><strong>64-bit integer(signed)</strong></td><td><strong>64 bit 有符号整形</strong></td><td><strong>torch.int64 or torch.long</strong></td><td><strong>torch.LongTensor</strong></td><td><strong>torch.cuda LongTensor</strong></td></tr><tr><td>Boolean</td><td>布尔</td><td>torch.bool</td><td>torch.BooleanTensor</td><td>torch.cuda.BooleanTensor</td></tr></tbody></table><h4 id="List-vs-Nparray">List vs Nparray</h4><p>Numpy中的Nparray采用连续地址存储，原生list只能通过寻址方式找到下一种元素；</p><p><img src="/article/2751853729/image-20240102223912335.png" alt=""></p><p>这是因为Numpy制定了其存储的数据类型，可以统一分配内存空间，而List中的数据类型是确定的。</p><ul><li><p>Nparray在科学计算方面性能远高于List, 可以省掉许多循环语句</p></li><li><p>Nparray支持并行化运算，底层采用C语言编写，接触了Python解释器的性能限制，所以效率远高于纯Python代码</p></li></ul><h4 id="Numpy-vs-Tensor">Numpy vs Tensor</h4><p>Numpy和Tensor相比较，他们的区别如下：</p><ul><li>Tensor 和 Numpy都是矩阵，区别是前者可以在GPU上运行，后者只能在CPU上。</li><li>Tensor可以直接通过print显示数据类型，而Numpy不可以。</li></ul><blockquote><p><strong>在GPU上运行时:</strong>  Tensor内部的数据类型为ndarray，GPU不具有更改元素值的能力，这时Tensor内部元素的数值不可改变</p></blockquote><h3 id="类型常见操作">类型常见操作</h3><ul><li><p><strong>将普通张量类型转化为GPU张量类型的方法</strong>: <code>普通张量变量名.cuda()</code>, 返回一个GPU张量的引用。</p></li><li><p><strong>Tensor默认的数据类型</strong></p><ul><li>默认的Tensor是FloatTensor（如果默认类型为GPU tensor，则所有操作都将在GPU上进行）。</li><li><strong>设置默认的数据类型</strong>: <strong><code>torch.set_default_tensor_type(类型名)</code></strong></li><li>增强学习中使用DoubleTensor的使用会更多</li></ul></li><li><p><strong>将张量转换为其他数据类型</strong></p><ul><li><strong>将张量转换为numpy数组</strong>：<code>张量名.numpy()</code></li><li><strong>将只有一个元素的张量转换为标量</strong>：<code>张量名.item()</code>。</li></ul></li><li><p><strong>查看张量数据类型的方法</strong></p><ul><li><strong>type方法</strong>：使用<code>张量名.type()</code>可以查看张量的具体类型。</li><li><strong>isinstance</strong>：isinstance是Python的自带函数。用法<code>isinstance(torch.randn(2,3),torch.FloatTensor)</code>，返回布尔值。</li></ul></li><li><p><strong>生成元素数据类型指定的张量</strong></p><p><strong>浮点型张量</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tensor.FloatTensor(标量/列表/numpy数组)  # 生成元素均为单精度浮点型的张量</span><br><span class="line">tensor.DoubleTensor(标量/列表/numpy数组) # 生成元素均为双精度浮点型的张量</span><br><span class="line">tensor.HalfTensor(标量/列表/numpy数组)   # 生成元素均为半精度浮点型的张量</span><br></pre></td></tr></tbody></table></figure><p><strong>整型张量</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tensor.IntTensor(标量/列表/numpy数组)    # 生成元素均为基本整型的张量</span><br><span class="line">tensor.ShortTensor(标量/列表/numpy数组)  # 生成元素均为短整型的张量</span><br><span class="line">tensor.LongTensor(标量/列表/numpy数组)   # 生成元素均为长整型的张量</span><br></pre></td></tr></tbody></table></figure><p><strong>布尔型张量</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tensor.BoolTensor(标量/列表/numpy数组)   # 生成元素均为布尔类型的张量</span><br></pre></td></tr></tbody></table></figure></li></ul><blockquote><p>[<strong>python、numpy、Pytorch中的索引方式</strong>](</p></blockquote><h2 id="Tensor的创建">Tensor的创建</h2><blockquote><p>加粗为常用</p></blockquote><h3 id="创建函数">创建函数</h3><table><thead><tr><th>函数</th><th>功能</th><th>备注</th></tr></thead><tbody><tr><td>Tensor(*size)</td><td>基础构造函数</td><td>size: 直接根据形状定义Tensor, 例：torch.tensor(标量|列表)，</td></tr><tr><td>Tensor(data)</td><td>类似np.array</td><td>data: 使用数据直接初始化,  例：torch.from_numpy(numpy数组)</td></tr><tr><td>*<em>ones(<em>size)</em></em></td><td><strong>全1Tensor</strong></td><td><strong>常用结构：全部为1的张量</strong></td></tr><tr><td>*<em>zeros(<em>size)</em></em></td><td><strong>全0Tensor</strong></td><td><strong>常用结构：全部为0的常量</strong></td></tr><tr><td>*<em>eye(<em>size)</em></em></td><td><strong>对角线为1，其他为0</strong></td><td><strong>常用结构：对角线为1，其他为0</strong></td></tr><tr><td>arange(s,e,step)</td><td>从s到e，步长为step</td><td>从s到e, 中间的间隔为step，即步长</td></tr><tr><td>linspace(s,e,steps)</td><td>从s到e，均匀切分成steps份</td><td>从s到e, 均匀切分成steps份</td></tr><tr><td>rand/randn(*size)</td><td>均匀/标准分布</td><td>size: 根据形状定义Tensor, <strong>值为随机赋值，均匀/标准分布的随机采样</strong></td></tr><tr><td>normal(mean,std)/uniform (from,to)</td><td>正态分布/均匀分布</td><td>满足<a href="https://zhuanlan.zhihu.com/p/514912456">正态分布或均匀分布</a></td></tr><tr><td>randperm(m)</td><td>随机排列</td><td>对一个序列进行随机排列</td></tr><tr><td>empty(*size)</td><td>生成不经过元素初始化的指定形状的张量</td><td></td></tr><tr><td>rand_like(tensor)</td><td>生成指定填充值的指定形状的张量</td><td></td></tr><tr><td>full(*size)</td><td>指定值的Tensor</td><td></td></tr></tbody></table><h3 id="编程实例"><strong>编程实例</strong></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">printTensor</span>(<span class="params">tensor</span>):</span><br><span class="line">    <span class="built_in">print</span>(tensor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"numel ="</span>, tensor.numel())  <span class="comment"># 输出9</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"dim ="</span>, tensor.dim())  <span class="comment"># 输出2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"type ="</span>, tensor.<span class="built_in">type</span>())  <span class="comment"># 输出2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''常用的Tensor定义'''</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------as_tensor--------------"</span>)</span><br><span class="line">shape = [[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>]]</span><br><span class="line">tensor = torch.as_tensor(shape)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------Tensor(* Size)--------------"</span>)</span><br><span class="line">shape = [(<span class="number">2</span>, <span class="number">3</span>), (<span class="number">4</span>, <span class="number">5</span>), [<span class="number">6</span>, <span class="number">7</span>]]</span><br><span class="line">tensor1 = torch.Tensor(shape)</span><br><span class="line">printTensor(tensor1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------numpy.array()--------------"</span>)</span><br><span class="line">data_array = np.array([[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line">tensor = torch.tensor(data_array)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个3行3列的张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------torch.ones--------------"</span>)</span><br><span class="line">tensor = torch.ones((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------torch.zeros--------------"</span>)</span><br><span class="line">tensor = torch.zeros(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------torch.zeros_like--------------"</span>)</span><br><span class="line">tensorlike = torch.zeros_like(tensor1)</span><br><span class="line">printTensor(tensorlike)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------torch.randn--------------"</span>)</span><br><span class="line">tensor = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="string">'''正态分布'''</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.normal mean为均值，std为标准差-1--------------"</span>)</span><br><span class="line"><span class="comment"># std=5组不同的正诚分布，5组都是随机的标准差和 mean =0</span></span><br><span class="line">tensor = torch.normal(mean=<span class="number">0.0</span>, std=torch.rand(<span class="number">5</span>))</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.normal mean为均值，std为标准差-2--------------"</span>)</span><br><span class="line"><span class="comment"># std=5组不同的正诚分布，5组都是随机的标准差和随机的mean</span></span><br><span class="line">tensor = torch.normal(mean=torch.rand(<span class="number">5</span>), std=torch.rand(<span class="number">5</span>))</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.uniform_--------------"</span>)</span><br><span class="line">tensor = torch.Tensor(<span class="number">4</span>, <span class="number">2</span>).uniform_()</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''定义一个序列'''</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.arange--------------"</span>)</span><br><span class="line"><span class="comment"># 定义一个序列, 步长为2， 最后10不包含在序列中</span></span><br><span class="line">tensor = torch.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.linspace--------------"</span>)</span><br><span class="line"><span class="comment"># 等间节切分,5为个数，11为范围，0为起始值</span></span><br><span class="line">tensor = torch.linspace(<span class="number">0</span>, <span class="number">11</span>, <span class="number">5</span>)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="Tensor的属性">Tensor的属性</h2><ul><li>每一个Tensor有torch.dtype、torch.device、torch.layout三种属性</li><li>torch.device 标识了torch.Tensor对象在创建之后所存储在的设备名称</li><li>torch.layout表示torch.Tensor内存布局的对象</li></ul><blockquote><p>torch.dtype： 在使用tensor函数创建tensor张量对象时还可以使用dtype参数指定数据类型</p><p>torch.device:  张量所创建的数据，到底应该存储在哪个设备上，CPU或GPU, GPU是通过CUDA来表示，多个则用cuda:0, cuda:1，以此类推</p><p>torch.layout:  张量的排布方式，对应到内存中连续的区别。稠密或稀疏的方式。</p></blockquote><h3 id="稠密的张量">稠密的张量</h3><p><strong>稠密的张量定义方法</strong></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># GPU</span></span><br><span class="line"><span class="comment"># GPU则代码改为： dev = torch.device("cuda:0")</span></span><br><span class="line">dev = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], dtype=torch.float32,device=dev)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></tbody></table></figure><h3 id="稀疏的张量"><strong>稀疏的张量</strong></h3><p><strong>稀疏或低秩</strong>是机器学习中两个很重要的概念，描述了当前数据是否满足某种性质</p><ul><li><p>稀疏表达了当前数据中，非0元素的个数，非0元素的个数越少，说明越稀疏。如果全部为0，则说明最稀疏。</p></li><li><p>低秩描述了数据本身的关联性，也是线性代码中的一个概念。<strong>秩</strong>从线性相关的角度来看，主要是描述了当前矩阵中的向量间线性可表示的关系。</p></li></ul><p><strong>稀疏的张量在机器学习中的优势</strong></p><ul><li>从模型角度：能够使<strong>模型</strong>变的非常简单。对于有参数的模型，如果参数中0的个数非常多，意味着可以对模型进行简化。即参数为0的项(item) 是可以减掉的，因为0乘以任何数都等于0。这样参数个数变少，意味着模型变的更简单，对于参数稀疏化的约束在机器学习中是一个非常重要的性质。这是我们从机器学习模型的角度上介绍稀疏的意义。</li><li>从数据角度，通过对数据进行稀疏化的表示，可以减少数据在内存中的开销。假设存在一个100*100的矩阵，哪果用稠密方式表示数据，则需要100**100单位的空间，而如果用稀疏张量表示，我们只要记住非0元素的坐标即可。</li></ul><p><strong><code>torch.sparse_coo_tensor</code></strong></p><p>PyTorch中用<code>torch.sparse_coo_tensor</code> 表示稀疏矩阵。使用<code>torch.sparse_coo_tenso</code>r可以方便地将稀疏矩阵转换为PyTorch张量，并进行各种操作。同时，由于只存储了非零元素的位置和值，因此可以节省大量的内存空间。</p><blockquote><p>名称是 'coo’代表非零元素的坐标。即<strong>coo类型</strong>:  coo类型表示了非零元素的坐标形式</p></blockquote><p><strong><code>torch.sparse_coo_tensor</code>参数说明</strong>：</p><ul><li><p>indices:  一个二维的LongTensor,  表示<strong>非零元素在原矩阵中的位置</strong>，其形状为(N, 2),其中N为非零元素个数。</p></li><li><p>values: 一个一维的Tensor, 表示<strong>非零元素的值</strong>，其形状为(N,)。</p></li><li><p>size: 一个元组，表示<strong>输出张量的形状</strong>，例如(M, N)。</p></li></ul><p><strong><code>torch.sparse_coo_tensor</code>参数解说</strong>：</p><ul><li>indices: 表示非零元素在原矩阵中的位置，即哪些位置是非零的。它的形状为(N, 2),  其中<strong>N为非零元素个数。每个元素包含两个值，分别表示该非零元素在行和列上的位置</strong>。</li><li>values： 表示非零元素的值，即<strong>这些位置上的数值</strong>。它的形状为(N,)</li><li>size： 表示输出张量的形状，即输出张量的行数和列数。例如，如果输入矩阵是一个4x5的矩阵，但是只有第2行和第4行、第3列和第5列上的元素是非零的，那么输出张量的形状就是(2, 2)。</li></ul><p><strong><code>torch.sparse_coo_tensor</code>用法</strong></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dev = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义长度分别为3个长度的坐标: [0,2] [1,0] [1,2],</span></span><br><span class="line"><span class="comment"># indices = torch.tensor([[0, 1, 2], [0, 1, 2]]) 会保存数据落地对角线上</span></span><br><span class="line">indices = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="comment"># 以上3组从坐标对应的三个非0元素 3,4,5</span></span><br><span class="line">values = torch.tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 原张量的形状是一个2,4的tensor, 如果用稠密方式打印，打看到一个2,4的变量</span></span><br><span class="line">x = torch.sparse_coo_tensor(indices, values, [<span class="number">3</span>, <span class="number">3</span>], device=dev, dtype=torch.float32)</span><br><span class="line">x_to_dense = x.to_dense()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------sparse_coo_tensor------------"</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------x_to_dense------------"</span>)</span><br><span class="line"><span class="built_in">print</span>(x_to_dense)</span><br></pre></td></tr></tbody></table></figure><p>控制台输出</p><p><img src="/article/2751853729/image-20240103020805785.png" alt=""></p><p><strong>例：将数据落地对角线上</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">indices = torch.tensor([[0, 1, 2], [0, 1, 2]]) </span><br><span class="line">values = torch.tensor([1,2,3], dtype=torch.float32)</span><br></pre></td></tr></tbody></table></figure><blockquote><p><strong>解释</strong>：</p><p>i的坐标 （0,0）（1,1）（2,2），第0行代表x，第1行代表y</p><p>矩阵的坐标：   [（0,0）  （0,1） （0,2）</p><p>​                           （1,0）  （1,1） （1,2）</p><p>​                           （2,0）  （2,1） （2,2） ]</p><p><strong>数据正好在对角线</strong></p></blockquote><h2 id="Tensor的算术运算">Tensor的算术运算</h2><p><strong>四则运算</strong></p><ul><li>加减乘除</li><li>矩阵运算</li></ul><p><strong>其他运算</strong></p><ul><li>torch.pow - 幂运算</li><li>torch.exp(input, out=None) - e指数，注意只支持浮点型</li><li>torch.sqrt(input, out=None) - 开方</li><li>torch.log(input, out=None) - 对数运算，以e为底</li><li>ceil/round/floor/trunc - 取整/四舍五入/下取整/只保留整数部分 - 如<code>torch.ceil(input, out=None)</code></li><li>clamp(input, min, max) - 超过min和max部分截断 - <code>torch.clamp(input, min, max, out=None)</code></li><li>torch.abs(input, out=None)- 求绝对值</li><li>…</li></ul><h3 id="加减法运算"><strong>加减法运算</strong></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">c = a + b  <span class="comment"># a - b ,</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line">c = torch.add(a, b)  <span class="comment"># 减法sub</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(a.add(b)) <span class="comment"># 减法sub，</span></span><br><span class="line"><span class="comment"># a.add_(b)  # sub_(...)， 带下划线的方式，运算后将结果同时赋给a。加减乘除等运算中含有下划线"_"的规则都一样。</span></span><br><span class="line"><span class="built_in">print</span>(a) </span><br></pre></td></tr></tbody></table></figure><blockquote><p>减法：- / …sub() / …sub_()<br>乘法: * / <code> c = torch.mul(a,b)</code>  /<code>a.mul(b)</code>  /<code>a.mul_(b)</code></p></blockquote><h3 id="哈达玛积">哈达玛积</h3><p><strong>哈达玛积(element wise，对应元素相乘) - mul乘法</strong> 如果一个tensor是shape是<code>2 * 2</code> 的话，则另外一个tensor也是<code>(2 * 2)</code>,  这样保证所有元素都保证每个元素都可以被相乘。</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">c = a * b</span><br><span class="line">c = torch.mul(a, b) <span class="comment"># a和b的shape要一样的</span></span><br><span class="line">a.mul(b)</span><br><span class="line">a.mul_(b) <span class="comment"># 同时将结果赋给a</span></span><br></pre></td></tr></tbody></table></figure><h3 id="矩阵运算">矩阵运算</h3><h4 id="二维矩阵的乘法运算">二维矩阵的乘法运算</h4><p>**二维矩阵的乘法运算 **包括<code>torch.mm(),torch.matmul(),@</code></p><p>规则<code> a * b</code> 时，即两个矩阵相乘，<code>m * n</code>,  <code>n * p</code>, 一定要保证 两个矩阵的’<strong>n</strong>’是相同的。</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">b = torch.ones(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.mm(a,b))</span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b))</span><br><span class="line"><span class="built_in">print</span>(a @ b)</span><br><span class="line"><span class="built_in">print</span>(a.matmul(b))</span><br><span class="line"><span class="built_in">print</span>(a.mm(b))</span><br></pre></td></tr></tbody></table></figure><h4 id="高维矩阵的乘法运算"><strong>高维矩阵的乘法运算</strong></h4><p>对于高维的<code>Tensor(dim&gt;2)</code>，假如矩阵的<code>size=(a1,a2,m,n)</code> ，我们要保证除最后两维<code>m,n</code>之外的前几维的值保持一致, 最后两维的规则同二维矩阵，即n相同 。就像矩阵的索引一样并且运算操只有``torch.matmul()`</p><blockquote><p>同样是除了矩阵内的数值之外，要保证维度的每个元素都可以被计算。<br><code>mm</code>()与<code>matmul()</code>不存在下划线类的计算.。</p></blockquote><p>如下所示：<code>n</code>相同，<code>a,b</code>相同。</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.ones(a, b, m, n)</span><br><span class="line">b = torch.ones(a, b, n, p)</span><br></pre></td></tr></tbody></table></figure><p>举例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.ones(1, 2, 4, 5)</span><br><span class="line">b = torch.ones(1, 2, 5, 3)</span><br><span class="line">print(a.matmul(b))</span><br><span class="line">print(torch.matmul(a, b))</span><br></pre></td></tr></tbody></table></figure><blockquote><p><a href="https://zhuanlan.zhihu.com/p/185249529">矩阵的乘除与逆运算</a></p></blockquote><h3 id="幂运算"><strong>幂运算</strong></h3><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.full((2, 3), fill_value=2)</span><br><span class="line">print(torch.pow(a, 2))</span><br><span class="line">print(a.pow(2))</span><br><span class="line">print(a ** 2)</span><br><span class="line">print(a.pow_(2))</span><br></pre></td></tr></tbody></table></figure><h3 id="指数运算"><strong>指数运算</strong></h3><p><strong>指数函数：<strong>函数y=a^x(a&gt;0且a≠1) 叫做</strong>指数函数</strong>，a是常数，x是自变量，定义域为R，值域为（0，+∞）。要求：a^x前的系数必须是数1，自变量x必须在指数的位置上，且不能是x的其他表达式，否则就不是指数函数。</p><img src="/article/2751853729/image-20240103195933889.png" alt="image-20240103195933889" style="zoom:67%;"><ul><li>a&gt;1时，则指数函数单调递增；若0&lt;a&lt;1，则为单调递减的。</li><li>对于a不大于0的情况，函数的定义域不连续，不考虑; a等于0函数无意义一般也不考虑。</li><li>指数函数恒过（0，1）点，即水平直线y=1是从递减到递增的一个过渡位置。</li><li>指数函数是非奇非偶函数。</li><li>…</li></ul><p><strong>指数函数应用到自然常数e上写为exp(x)，现常写为e^x</strong>（表示为x=lny），其图像是单调递增，n∈R，y&gt;0，与y轴相交于（0,1）点。，图像位于X轴上方，第二象限无限接近X轴。</p><h4 id="e的n次方">e的n次方</h4><p>$$<br>y = e^n \ n∈R，y&gt;0，与y轴相交于(0,1)点<br>$$</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.full((2, 3), fill_value=2)</span><br><span class="line">print(torch.exp(a))</span><br></pre></td></tr></tbody></table></figure><h3 id="开方运算"><strong>开方运算</strong></h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.full((<span class="number">2</span>, <span class="number">3</span>), fill_value=<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a.sqrt())</span><br><span class="line"></span><br><span class="line"><span class="comment"># ----返回值-----</span></span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]])</span><br></pre></td></tr></tbody></table></figure><h3 id="对数运算"><strong>对数运算</strong></h3><p>对数源于指数，是指数函数反函数 因为：N = a<sup>x&nbsp;</sup>  所以：x = log(a<sup>N</sup>)</p><p>如果 N=a^x（a&gt;0,a≠1），即<em>a</em>的<em>x</em>次方等于<em>N</em>（<em>a</em>&gt;0，且<em>a</em>≠1），那么数<em>x</em>叫做以<em>a</em>为底<em>N</em>的对数（logarithm），记作：x=log（a<sup>N</sup>）　其中，<em>a</em>叫做对数的底数，<em>N</em>叫做真数，<em>x</em>叫做 “以<em>a</em>为底<em>N</em>的对数”。</p><blockquote><ul><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzA1NDkzMg==&amp;mid=2651391217&amp;idx=1&amp;sn=ca5a92928be3100b9f8565931b3c386f">对数运算规则</a></li><li><a href="https://blog.csdn.net/weixin_39852647/article/details/110869824">数据增长率怎么算</a> -  对数是用来衡量增长率的: 对数函数中，x是自变量，y是因变量，当x大于1时，对数函数是增函数，所以取对数就是增长率。</li></ul></blockquote><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.full((2, 3), fill_value=4)</span><br><span class="line">print(torch.log2(a))</span><br><span class="line">print(torch.log10(a))</span><br><span class="line">print(torch.log(a))</span><br></pre></td></tr></tbody></table></figure><h2 id="Tensor的更多操作">Tensor的更多操作</h2><blockquote><p><a href="https://blog.csdn.net/a171232886/article/details/121458916"><strong>python、numpy、Pytorch中的索引方式</strong></a></p></blockquote><h3 id="维度调整">维度调整</h3><ul><li><p>查看维度：<code>torch.shape</code></p></li><li><p>变换维度：<code>torch.reshape([d0,d1,d2])</code></p></li><li><p><code>unsqueeze</code>和<code>squeeze</code></p><ul><li><p><code>b=a.squeeze(a)</code> 去掉维度值为1的维度</p></li><li><p><code>b=a.unsqueeze(n)</code> 增加一个维度在shape的第n个</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">c = torch.rand(2, 3)</span><br><span class="line">c = c.unsqueeze(1)</span><br><span class="line">print(c.shape)</span><br></pre></td></tr></tbody></table></figure></li></ul></li></ul><h3 id="两个tensor合并">两个tensor合并</h3><ul><li><code>torch.cat(tensors, dim=0, out=None)</code></li><li><code>torch.stack(tensors)</code>合并时候新建一个维度</li></ul><h3 id="与numpy互换">与numpy互换</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/352877584">图解PyTorch中的torch.gather函数 - 知乎</a></li><li><a href="https://blog.csdn.net/lilai619/article/details/123083492">pytorch和numpy的互转</a> ，有以下几种方式：<ul><li><code>torch.gather</code>对应的<code>numpy</code>操作：</li><li><code>torch.view</code>对应的<code>numpy</code>操作</li><li><code>torch.argmax</code>对应的<code>numpy</code>操作</li><li><code>torch.max</code>对应的<code>numpy</code>操作</li></ul></li></ul><p><strong>参考</strong></p><ul><li><a href="https://blog.csdn.net/hanmo22357/article/details/129524092">https://blog.csdn.net/hanmo22357/article/details/129524092</a></li><li><a href="https://blog.csdn.net/a171232886/article/details/123900387">https://blog.csdn.net/a171232886/article/details/123900387</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> PyTorch系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch入门-机器学习基础</title>
      <link href="/article/2348287028.html"/>
      <url>/article/2348287028.html</url>
      
        <content type="html"><![CDATA[<h2 id="分类与回归问题">分类与回归问题</h2><p>机器学习要解决的问题，主要还是围绕的“<strong>分类与回归</strong>”两大问题来展开。</p><ul><li>图像识别：大部分会归为分类任务</li><li>目标检测:  找到图片中的位置，同时找到类别，即属于分类问题与属于回归问题</li></ul><h3 id="分类问题还是回归问题的判断">分类问题还是回归问题的判断</h3><p><img src="/article/2348287028/image-20240102030421021.png" alt=""></p><p><strong>分类任务是指用离散的值来描述的任务</strong>，如上左图，一共有10分类别，对应有10维禹量。假如某分图片的10维向量为：<code>[0.1,0.1,0.10.1,0,0,0,0.5,0.1,0]</code> , 其中<strong>0.5</strong>的概率最大，而0.5在该向量中位于第8的位置，那说明该图像的分类为"dog"。 总之：我们预测10类别，对应有总和为1的10维向量，我们就称为该向量为10维向量,其中每个向量的索引值即为类别的编号。 同量，如果我们要预测1000个类别，则输入为1000维向量，同样索引位置即为类别编号,  总和为1。</p><p><strong>与分类任务相对的则是回归任务，输出为连续值，而不是离散值</strong>，如上右图，是一个股票价格的预测、房价的预测、身高的预测等，解决连续值的预测。因此我们可以这么定义回归任务：如果我们网络或采用机器学习的模型最终输出的是一个连续值，那称之为回归任务。</p><h2 id="机器学习构成元素">机器学习构成元素</h2><p>机器学习围绕 <strong>样本、模型、训练、测试、推理</strong> 这五个元素开展工作。</p><h3 id="样本">样本</h3><p>样本是获取知识的依据（数据）。对于机器学习问题而言，并不是进行盲目的进行推理，一般是根据已知的内容来抽取出一个客观的规律，再根据规律进行推理与预测。这些规律就是从样本挖掘而出，对应的机器学习中的 <strong>学习</strong></p><p>样本包括以下两样内容：</p><ul><li>属性：描述样本本身的性质</li><li>标签：样本的类别，可以是离散和也可以是连续的</li></ul><p>可以用<code>y = f(x)</code>函数来表示属性与样本之间的关系，其中x为属性，y为标签。标签是由属性归纳总结而来。</p><h3 id="模型">模型</h3><p>有了样本（x,y) 之后，就可以用模型来将规律挖出，模型就对应<code>y =f(x)</code>函数中的<code>f</code>，即<code>f</code>为函数关系，通过挖掘样本属性之间的内在联系，来得到样本的标签。模型的公式：<code>f(x) = wx + b</code></p><h3 id="训练">训练</h3><p>学习这个模型的过程，就是所谓的训练过程。也就是求解w,b过程，其中w和b称为参数。</p><blockquote><p>求解参数的方法很多，用PyTorch主要是用深度学习的方式来实现。即通过训练，能够求解出模型中的参数。</p></blockquote><h3 id="测试">测试</h3><p>通过训练，能够求解出模型中的参数。训练得到参数后，就可以对当前的模型进行测试与评估。我们通过机器学习的方法计算出的一组参数，我们认为这组参数是最优的，但到底有多好，是否还有其他方法求解出来的参数是否比之训练得到的模型参数最终预测结果会更好呢？这个时候我们会通过一套测试方法来完成。总之通过测试我们能够完成对模型性能的评估。</p><p>分类：ROC曲线</p><p>评价：PR曲线进行评价</p><blockquote><p><strong>PR 曲线</strong>是一种用于评估二分类模型性能的可视化工具。它以查准率为纵轴，召回率为横轴绘制曲线。</p><p><strong>ROC曲线</strong>全称是"Receiver Operating Characteristic"曲线，被广泛用于评价分类器的性能。</p></blockquote><h3 id="推理">推理</h3><p>如果性能OK,，则就有了<code>f(x)</code>函数, 其中<code>f</code>为训练后的模型。现拿到一组样本，该样本只有属性没有标签，我们利于<code>f(x)</code>函数，对该样本进行推理，获取标签。利于学习所得的模型来获取样本标签的过程称为推理。</p><h2 id="机器学习的泛化与优化">机器学习的泛化与优化</h2><p><strong>优化(optimization)</strong>: 是指调节模型以在训练数据上得到最佳性能，即机器学习中的学习</p><p><strong>泛化(generalization)</strong>：是指训练好的模型在前所未见的数据上的性能好坏。</p><p><strong>泛化能力</strong>：根据泛化能力强弱可以分为:</p><ul><li><strong>欠拟合</strong>: 模型过于简单，不能在训练集上获得足够低的误差</li><li><strong>拟合</strong>:  测试误差与训练误差差距较小;</li><li><strong>过拟合</strong>: 过分关注训练集细节，在训练集上表现良好，但不能泛化到新数据上</li><li><strong>不收敛</strong>: 模型不是根据训练集训练得到的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> PyTorch系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch零基础入门</title>
      <link href="/article/1499102559.html"/>
      <url>/article/1499102559.html</url>
      
        <content type="html"><![CDATA[<h2 id="PyTorch介绍">PyTorch介绍</h2><p><img src="/article/1499102559/image-20240103050628116.png" alt=""></p><p>PyTorch是Facebook人工智能研究员(FAIR)于2017年在Github上开源，前身是Torch。Torch并非是用主流语言编写，考虑到AI行业主流语言与受众群体，factbook用Python对Torch框架进行重构，虽然pytorch有某些底层功能也用到了c/c++写的，但pytorch都给这些功能提供了python的接口，这样可以让程序员很方便的用python调用PyTorch整个框架的功能，以此来完成深度学习的模型搭建，并利用这些模型来解决实际问题。</p><p><strong>Pytorch vs TensorFlow</strong></p><table><thead><tr><th><strong>Pytorch</strong></th><th><strong>TensorFlow</strong></th></tr></thead><tbody><tr><td>简洁（编程同Python几乎一致） <br>动态计算<br>visdom<br>部署不方便</td><td>接口复杂 <br>静态图（TF2.0 Eager Execution已引入动态图）<br>Tensorboard (现阶段PyTorch也引入了) <br>部署方便(TF serving)<br></td></tr></tbody></table><blockquote><p><strong>什么是动态图与静态图</strong></p><ul><li><p>动态图: 编好程序即可执行，</p></li><li><p>静态图: 先搭建计算图，后运行，允许编译器进行优化</p></li></ul></blockquote><p><strong>部署方式</strong></p><ul><li><p>Pytorch如果要做部署，需要用到Python的fastAPI等来完成部（也可直接用Starlette。FastAPI也是基于Starlette，适用于高并发）</p></li><li><p>TensorFlow的TF serving直接完成部署，在资源调度上更好一些，在模型部署时比较好。</p></li><li><p>学习建议：时间足够的话，两者都学习；时间不够，学pytorch。</p></li></ul><p><strong>torch编程</strong></p><blockquote><p>编程与调试都比较简单</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">B = torch.Tensor([<span class="number">10</span>])</span><br><span class="line"><span class="keyword">while</span>(A&lt;B)[O]:</span><br><span class="line">   A += <span class="number">2</span></span><br><span class="line">   B+=<span class="number">1</span></span><br><span class="line">   <span class="built_in">print</span>(A)</span><br><span class="line">   <span class="built_in">print</span>(B)</span><br></pre></td></tr></tbody></table></figure><p><strong>TensorFlow编程:</strong></p><blockquote><p>静态图在编译时，可以被编译器进行一些优化操作; 代码编译复杂，调试不直观</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">A = tf.constant(<span class="number">0</span>)</span><br><span class="line">B = tf.constant(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cond</span>(<span class="params">A, B,*args</span>):</span><br><span class="line">    <span class="keyword">return</span> A &lt; B</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">body</span>(<span class="params">A, B</span>)</span><br><span class="line">A = tf.add(A, <span class="number">2</span>)</span><br><span class="line">B = tf.add(B <span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> A, B</span><br><span class="line"></span><br><span class="line">c1, c2 = tf.while_loop(cond, body,[A,B])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">A res, B res = sess.run([c1, c2])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A res)</span><br><span class="line"><span class="built_in">print</span>(B res)</span><br></pre></td></tr></tbody></table></figure><h2 id="安装与搭建PyTorch的环境">安装与搭建PyTorch的环境</h2><p>笔者的文章：<a href="https://juejin.cn/post/7290494120207319092">Python+PyTorch+Anaconda安装配置</a></p><h2 id="机器学习基础">机器学习基础</h2><p><a href="https://blog.nianxi.cc/article/2348287028.html">机器学习基础</a></p><h2 id="PyTorch基本概念">PyTorch基本概念</h2><p>三个基本概念：Tensor、Variable(autograd), nn.Module，如下图所示</p><p><img src="/article/1499102559/image-20240102170233762.png" alt=""></p><h3 id="Tensor">Tensor</h3><p><a href="https://blog.nianxi.cc/article/2348287033.html">Tensor的基本概念</a></p><h3 id="Variable-autograd">Variable (autograd)</h3><p>变量在机器学习中是一个非常重要的概念，在我们的机器学习中，我们需要用变量来表达参数。在机器学习中，我们想要去求解的模型，刚开始模型参数是未知的，这些未知的参数就是<strong>变量</strong>。</p><h3 id="nn-Module">nn.Module</h3><p>nn.Module用来解决计算机视觉等任务会用到一些网络结构，这些网络结构是搭建深度学习、模型所需要用到的一个个积木元素。</p>]]></content>
      
      
      <categories>
          
          <category> PyTorch系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java能力框架之JVM原理</title>
      <link href="/article/578523837.html"/>
      <url>/article/578523837.html</url>
      
        <content type="html"><![CDATA[<h2 id="JVM模型-一次编译，到处运行">JVM模型: 一次编译，到处运行</h2><pre class="mermaid">graph TDA[JAVA程序] --&gt;  B[JAVA字节码]B[JAVA字节码] -- 解释 --&gt; C[Window Java虚拟机] --机器码--&gt; E[windows操作系统]B[JAVA字节码] -- 解释 --&gt; D[Linux Java虚拟机] --机器码--&gt; F[Linux操作系统]</pre><p>java程序经过一次编译之后，将java代码编译为字节码也就是class文件，然后在不同的操作系统上依靠不同的java虚拟机进行解释，最后再转换为不同平台的机器码，最终得到执行。</p><h2 id="java程序执行流程">java程序执行流程</h2><pre class="mermaid">graph LRA[JAVA代码] --&gt; B[JAVA字节码-class文件] --&gt; C[JAVA HelloWorld] --&gt; D[加载配置JVM.cfg] --&gt; E[根据JVM.cfg找到jvm.dll] --&gt; F[初始化JVM,运行Java程序]  --&gt; G[找到main方法并运行]</pre><h3 id="编译Java代码">编译Java代码</h3><p>java代码通过编译之后生成字节码文件（class文件）。这个过程由JAVA编译器完成。</p><h3 id="运行-java-HelloWorld-指令">运行 <code>java HelloWorld</code> 指令</h3><p>运行<code>java HelloWorld</code>执行java程序，java会根据系统版本找到jvm.cfg，由该文件找到对应的JVM编译命令，对java程序进行编译。其中<code>$\JAVA_HOME\jre\lib${amd64}\jvm.cfg</code> 大致内容长这样：</p><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Copyright (c) 2003, 2013, Oracle and/or its affiliates. All rights reserved.</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">You may also <span class="keyword">select</span> a JVM <span class="keyword">in</span> an arbitrary location with the</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">"-XXaltjvm=&lt;jvm_dir&gt;"</span> option, but that too is unsupported</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">and may not be available <span class="keyword">in</span> a future release.</span> </span><br><span class="line"></span><br><span class="line">-Xms 1024</span><br><span class="line">-server KNOWN</span><br><span class="line">-client IGNORE</span><br></pre></td></tr></tbody></table></figure><blockquote><p>server KNOWN 和 client IGNORE：分别说明当前JVM运行于服务端模型，而不是客户端模型 即 server KNOWN 就表示名称为server的jvm可用, 且如笔者主机下的地方找到对应的jvm.dll<br>…\JetBrains\IntelliJ IDEA…\jbr\bin\server\jvm.dll<br>…\Java\jre\bin\server\jvm.dll …</p></blockquote><h3 id="JVM-cfg">JVM.cfg</h3><p>jvm.dll则是java虚拟机的主要实现。</p><h3 id="运行程序">运行程序</h3><p>通过JNI接口（它还常用于java与操作系统、硬件交互），找到class文件后并装载进JVM，然后找到main方法，最后执行。</p><h2 id="JVM基本结构">JVM基本结构</h2><p>JVM是可运行Java代码的假想计算机 ，包括一套字节码指令集、一组寄存器、一个栈、一个垃圾回收，堆 和 一个存储方法域。JVM是运行在操作系统之上的，它与硬件没有直接的交互。下图是JVM的基本结构：</p><p><img src="/article/578523837/image-20240101193802898.png" alt=""></p><p>class文件被jvm装载以后，经过jvm的内存空间调配，最终是由执行引擎完成class文件的执行。当然这个过程还有其他角色模块的协助，这些模块协同配合才能让一个java程序成功的运行，下面就详细介绍这些模板，它们也是后面<strong>学习jvm最重要的部分</strong>。</p><h3 id="内存空间">内存空间</h3><p>JVM内存空间包含：方法区、java堆、java栈、本地方法栈。</p><h3 id="方法区">方法区</h3><p>方法区是各个线程共享的区域，存放类信息、常量、静态变量。</p><h3 id="java堆">java堆</h3><p>java堆也是线程共享的区域，我们的类的实例就放在这个区域。</p><p>可以想象你的一个系统会产生很多实例，因此java堆的空间也是最大的。如果java堆空间不足了，程序会抛出OutOfMemoryError异常。</p><p><strong>堆内存划分</strong>：</p><ul><li><p>JDK 1.8和以前<br>在JDK7以及其前期的JDK版本号中。堆内存通常被分为三块区域Nursery内存(young generation)、长时内存(old generation)、永久内存(Permanent Generation for VM Matedata)。</p></li><li><p>JDK 1.8以后<br>JDK 1.8以后Java 堆主要分为2个区域-年轻代与老年代，年轻代包括Eden 区和 Survivor 区，Survivor 区又分From区和 To区。</p></li></ul><blockquote><p>JDK 1.8以后不再用永久带，永久代已经不存在，存储的类信息、编译后的代码数据等已经移动到了元空间（MetaSpace）中，元空间并没有处于堆内存上，而是直接占用的本地内存（NativeMemory）。</p></blockquote><h3 id="java栈">java栈</h3><p>栈也叫栈内存，主管Java程序的运行，是在线程创建时创建，一个线程对应一个java栈，每执行一个方法就会往栈中压入一个元素。它的生命期是跟随线程的生命期，线程结束栈内存也就释放，对于栈来说不存在垃圾回收问题，只要线程一结束该栈就Over，生命周期和线程一致，是线程私有的。</p><p>往栈中压入一个元素叫“栈帧”，而栈帧中包括了方法中的局部变量、用于存放中间状态值的操作栈。</p><blockquote><p>如果java栈空间不足了，程序会抛出StackOverflowError异常<br>想一想什么情况下会容易产生这个错误：<strong>递归</strong>，递归如果深度很深，就会执行大量的方法，方法越多java栈的占用空间越大。</p></blockquote><h4 id="栈运行原理">栈运行原理</h4><p>栈中的数据都是以栈帧（Stack Frame）的格式存在，栈帧是一个内存区块，是一个数据集，是一个有关方法和运行期数据的数据集。当一个方法A被调用时就产生了一个栈帧F1，并被压入到栈中，A方法又调用了B方法，于是产生栈帧F2也被压入栈，B方法又调用了C方法，于是产生栈帧F3也被压入栈…… 依次执行完毕后，先弹出后进…F3栈帧，再弹出F2栈帧，再弹出F1栈帧。其顺序遵循<strong>先进后出</strong>、<strong>后进先出</strong>原则。</p><h4 id="帧的组成">帧的组成</h4><ol><li><p>局部变量表(Local Stack Frame)：是一组变量值的存储空间，包括方法参数和局部变量，其中方法参数按照声明顺序严格放置，局部变量可任意放置。</p></li><li><p>操作数栈(Operand Stack):  也称为操作栈，是一个后入先出栈。在Class 文件的Code 属性的 max_stacks 指定了执行过程中最大的栈深度。</p></li><li><p>动态链接(Dynamic Linking):  每个栈帧都包含一个执行运行时常量池中该栈帧所属方法的引用，该引用可支持方法调用过程中的动态连接(Dynamic Linking)。</p></li><li><p>返回地址(Return Address)： 当一个方法被执行后，有两种方式退出该方法：执行引擎遇到了任意一个方法返回的字节码指令或遇到了异常，并且该异常没有在方法体内得到处理。方法退出的过程实际上等同于把当前栈帧出栈。</p></li></ol><h3 id="本地方法栈">本地方法栈</h3><p>本地方法栈角色和java栈类似，只不过它是用来表示执行本地方法的，本地方法栈存放的方法调用本地方法接口，最终调用本地方法库，实现与操作系统、硬件交互的目的。</p><h3 id="PC寄存器">PC寄存器</h3><p>PC寄存器的作用就是控制程序指令的执行顺序。执行引擎就是根据PC寄存器调配的指令顺序，依次执行程序指令。</p><blockquote><p>“类已经加载了，实例对象、方法、静态变量都去了自己该去的地方"，那么程序该怎么执行，哪个方法先执行，哪个方法后执行，这些指令执行的顺序就是PC寄存器在管。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Java系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java能力框架之JVM调优</title>
      <link href="/article/1531152183.html"/>
      <url>/article/1531152183.html</url>
      
        <content type="html"><![CDATA[<p>操作系统级别Java进程所占用的内存数值不能准确的反应堆内存的真实占用情况，因为GC过后这个值是不会变化的，内存调优的时候要更多地使用JDK提供的内存查看工具，比如JConsole和Java VisualVM。</p><h2 id="Full-GC">Full GC</h2><h3 id="Full-GC的几种情况">Full GC的几种情况</h3><p>对JVM内存的系统级的调优主要的目的是减少GC的频率和Full GC的次数，过多的GC和Full GC是会占用很多的系统资源（主要是CPU），影响系统的吞吐量。特别要关注Full GC，因为它会对整个堆进行整理，导致Full GC一般由于以下几种情况：</p><ul><li>旧生代空间不足<br>调优时尽量让对象在新生代GC时被回收、让对象在新生代多存活一段时间和不要创建过大的对象及数组避免直接在旧生代创建对象。</li><li>Pemanet Generation空间不足</li><li>增大Perm Gen空间，避免太多静态对象</li><li>统计得到的GC后晋升到旧生代的平均大小大于旧生代剩余空间</li><li>控制好新生代和旧生代的比例</li><li>System.gc()被显示调用</li><li>垃圾回收不要手动触发，尽量依靠JVM自身的机制</li></ul><p>###调优比例不良设置会导致的后果</p><p>调优手段主要是通过控制堆内存的各个部分的比例和GC策略来实现，各部分比例不良设置会导致什么后果。</p><h4 id="新生代设置过小，有以下两种情况">新生代设置过小，有以下两种情况</h4><ul><li>会让新生代GC次数非常频繁，增大系统消耗；</li><li>会导致大对象直接进入旧生代，占据了旧生代剩余空间，诱发Full GC。</li></ul><h4 id="新生代设置过大，有以下两种情况">新生代设置过大，有以下两种情况</h4><ul><li>会导致旧生代过小（堆总量一定），从而诱发Full GC；</li><li>新生代GC耗时大幅度增加，一般说来新生代占整个堆1/3比较合适；</li></ul><h4 id="Survivor设置过小">Survivor设置过小</h4><p>导致对象从eden直接到达旧生代，降低了在新生代的存活时间。</p><h4 id="Survivor设置过大">Survivor设置过大</h4><p>导致eden过小，增加了GC频率。</p><h2 id="调优策略">调优策略</h2><p>由内存管理和垃圾回收可知新生代和旧生代都有多种GC策略和组合搭配，选择这些策略对于我们这些开发人员是个难题，JVM提供两种较为简单的GC策略的设置方式：</p><h3 id="吞吐量优先">吞吐量优先</h3><p>JVM以吞吐量为指标，自行选择相应的GC策略及控制新生代与旧生代的大小比例，来达到吞吐量指标。这个值可由<code>-XX:GCTimeRatio=n</code>来设置</p><h3 id="暂停时间优先">暂停时间优先</h3><p>JVM以暂停时间为指标，自行选择相应的GC策略及控制新生代与旧生代的大小比例，尽量保证每次GC造成的应用停止时间都在指定的数值范围内完成。这个值可由<code>-XX:MaxGCPauseRatio=n</code>来设置。</p><h2 id="JVM参数设置">JVM参数设置</h2><h3 id="堆内存">堆内存</h3><ul><li>-Xms:  或-XX:InitialHeapSize，最小堆内存,单位是Byte,m,g等</li><li>-Xmx：或 -XX:MaxHeapSize，最大堆内存,单位是Byte,m,g等<br>比如<code>java -Xms128m -Xmx2g </code></li></ul><h3 id="新生代">新生代</h3><ul><li><p>-XX:NewSize  初始时年轻区内存，通常为 Xmx 的 1/3 或 1/4。</p></li><li><p>-XX:MaxNewSize 最大年轻区内存</p></li><li><p>-XX:MaxTenuringThreshold<br>来控制新生代存活时间，尽量让对象在新生代被回收。 新生代=Eden + 2 个 Survivor空间, 但实际可用空间为 = Eden + 1 个 Survivor，即 90%。</p></li></ul><h3 id="新老比值变化">新老比值变化</h3><ul><li><p>-XX:NewRatio 指定老年代与新生代的堆大小比例。在使用CMS收集器时，此参数失效</p></li><li><p>-XX:SurvivorRatio<br>新生代中 Eden 与 Survivor 的比值。默认值为 8,即 Eden 占新生代空间的 8/10，另外两个 Survivor 各占 1/10。</p></li><li><p>-MinHeapFreeRatio<br>指定jvm heap在使用率小于所设定的值，heap进行收缩，Xmx==Xms的情况下无效</p></li><li><p>-XX:MaxHeapFreeRatio<br>指定jvm heap在使用率大于n的情况下,heap进行扩张,Xmx==Xms的情况下无效</p></li></ul><h3 id="线程栈">线程栈</h3><ul><li>-Xss：线程栈大小</li></ul><h3 id="Java-heap页大小">Java heap页大小</h3><ul><li>-XX:LargePageSizeInBytes：指定Java heap的分页页面大小</li></ul><h3 id="压缩类指针">压缩类指针</h3><ul><li><p>-XX:+UseCompressedClassPointers<br>压缩类指针。对象的类指针（_klass）被压缩至32bit，使用类指针压缩空间的基地址</p></li><li><p>-UseCompressedOops<br>压缩对象指针，oops是普通对象指针，Java堆中对象的对象指针被压缩到32bit，使用堆基地址。</p></li></ul><blockquote><p>注意：64bit的服务器上设置-Xmx32g时，-XX:+UseCompressedOops和-XX:+UseCompressedClassPointers会失效，所以最大的堆设置为<strong>31g</strong>。</p></blockquote><h3 id="永久代-jdk8以前">永久代(jdk8以前)</h3><ul><li>-XX:PermSize 永久代初始大小</li><li>-XX:MaxPermSize：永久代大小的最大值</li></ul><h3 id="元数据相关-jdk8及以后">元数据相关(jdk8及以后)</h3><ul><li><p>-XX:MetaspaceSize 初始空间大小，达到该值就会触发垃圾收集进行类型卸载。</p></li><li><p>-XX:MaxMetaspaceSize 最大空间，默认是没有限制的。</p></li><li><p>-XX:MinMetaspaceFreeRatio 在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集</p></li><li><p>-XX:MaxMetaspaceFreeRatio 在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集</p></li><li><p>-XX:MaxMetaspaceExpansion Metaspace增长时的最大幅度</p></li><li><p>-XX:MinMetaspaceExpansion  Metaspace增长时的最小幅度</p></li></ul><h3 id="垃圾回收统计信息">垃圾回收统计信息</h3><ul><li>-XX:+PrintGC</li><li>-XX:+PrintGCDetails</li><li>-XX:+PrintGCTimeStamps</li><li>-Xloggc:filename</li></ul><h3 id="回收器设置">回收器设置</h3><ul><li>-XX:+UseSerialGC: 设置串行收集器</li><li>-XX:+UseParallelGC: 设置并行收集器</li><li>-XX:+UseParalledlOldGC:设置并行年老代收集器</li><li>-XX:+UseConcMarkSweepGC:设置并发收集器</li></ul><h3 id="并行收集器设置">并行收集器设置</h3><ul><li><p>-XX:ParallelGCThreads<br>指定并行 GC 线程的数量。默认情况下，当 CPU 数量小于8，ParallelGCThreads 的值等于 CPU 数量，当 CPU 数量大于 8 时，则使用公式：<code>ParallelGCThreads = 8 + ((N - 8) * 5/8) = 3 +（（5*CPU）/ 8）</code>。</p></li><li><p>-XX:MaxGCPauseMillis:设置并行收集最大暂停时间</p></li><li><p>-XX:GCTimeRatio: 设置垃圾回收时间占程序运行时间的百分比<code>公式为1/(1+n)</code>。</p></li><li><p>-XX:+CMSIncrementalMode: 设置为增量模式，适用于单CPU情况。</p></li></ul><h3 id="生成堆内存快照">生成堆内存快照</h3><ul><li>-XX:+HeapDumpOnOutOfMemoryError：让JVM在发生内存溢出时自动的生成堆内存快照。</li><li>-XX:HeapDumpPath：快照存储路径，默认保存在JVM的启动目录下名为java_pid<pid>.hprof</pid></li><li>-XX:OnOutOfMemoryError: 异常发生时执行一些操作</li></ul><h3 id="代码缓存">代码缓存</h3><ul><li><p>-XX:InitialCodeCacheSize</p></li><li><p>-XX:ReservedCodeCacheSize<br>代码缓存确实很少引起性能问题，但是一旦发生其影响可能是毁灭性的。如果代码缓存被占满，JVM会打印出一条警告消息，并切换到interpreted-only 模式：JIT编译器被停用，字节码将不再会被编译成机器码。应用程序将继续运行，但运行速度会降低一个数量级，直到有人注意到这个问题。</p></li><li><p>-XX:+UseCodeCacheFlushing</p></li></ul><p>当代码缓存被填满时让JVM放弃一些编译代码，避免当代码缓存被填满的时候JVM切换到interpreted-only 模式 。</p><br><h2 id="后记">后记</h2><p>该类文章网上已经非常多，但是能完整的全面且清晰的写下来的，需要精力与耐力。为了JYM，笔者多写一文。</p><p><br><br></p><p><strong>参考</strong></p><p><a href="http://niweiwei.iteye.com/blog/2123347">http://niweiwei.iteye.com/blog/2123347</a></p><p><a href="https://blog.csdn.net/huaweitman/article/details/50552960">https://blog.csdn.net/huaweitman/article/details/50552960</a></p><p><a href="http://www.cnblogs.com/redcreen/archive/2011/05/05/2038331.html%EF%BC%8Cjvm%E8%B0%83%E4%BC%98%E5%8F%82%E8%80%83%E9%85%8D%E7%BD%AE">http://www.cnblogs.com/redcreen/archive/2011/05/05/2038331.html，jvm调优参考配置</a></p><p><a href="http://ifeve.com/useful-jvm-flags-part-4-heap-tuning/">http://ifeve.com/useful-jvm-flags-part-4-heap-tuning/</a></p><p><a href="https://blog.csdn.net/liubenlong007/article/details/78143285">https://blog.csdn.net/liubenlong007/article/details/78143285</a></p><p><a href="https://www.cnblogs.com/redcreen/archive/2011/05/04/2037057.html">https://www.cnblogs.com/redcreen/archive/2011/05/04/2037057.html</a></p><p><a href="https://www.cnblogs.com/zhulin-jun/p/6516292.html">https://www.cnblogs.com/zhulin-jun/p/6516292.html</a></p><p><a href="https://blog.csdn.net/beautygao/article/details/79083058">https://blog.csdn.net/beautygao/article/details/79083058</a></p><p><a href="https://blog.csdn.net/liubenlong007/article/details/78143285">https://blog.csdn.net/liubenlong007/article/details/78143285</a></p>]]></content>
      
      
      <categories>
          
          <category> Java系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sarah小站首页</title>
      <link href="/article/453215639.html"/>
      <url>/article/453215639.html</url>
      
        <content type="html"><![CDATA[<p>欢迎来到Sarah小站</p><p>姓名： Sarah</p><p>职业： 学生</p><p>与博主关系：家人</p>]]></content>
      
      
      <categories>
          
          <category> Sarah小站 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sarah </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java能力框架</title>
      <link href="/article/2914746800.html"/>
      <url>/article/2914746800.html</url>
      
        <content type="html"><![CDATA[<p>JAVA从初学到资深再到专家，是一个循序渐进的过程，不仅要coding，还要对IT知识体系有认知。以下是笔者认为高阶JAVA技术人员需要掌握的整体能力框架。相关内容的掌握的深浅直接反映其技术水平。</p><blockquote><p>改为文字，这样后面调整好修改</p></blockquote><h2 id="基础知识">基础知识</h2><h3 id="计算机基础">计算机基础</h3><div class="mermaid">  graph TD计算机基础 --&gt; 操作系统计算机基础 --&gt; 数据结构计算机基础 --&gt; 网络计算机基础 --&gt; 算法</div><h3 id="Java">Java</h3><div class="mermaid">  graph TDJava --&gt; JVMJava --&gt; 语言特性Java --&gt; 多线程Java --&gt; IO编程</div><h2 id="项目经验">项目经验</h2><div class="mermaid">  graph TD项目经验 --&gt; 项目描述项目经验 --&gt; 项目难点项目经验 --&gt; 项目问题项目经验 --&gt; 项目改进项目经验 --&gt; 理论知识理论知识 --&gt; WEB理论知识 --&gt; 敏捷</div><h2 id="架构能力">架构能力</h2><h3 id="基础架构能力">基础架构能力</h3><div class="mermaid">  graph TD基础架构能力 --&gt; Docker基础架构能力 --&gt; K8S基础架构能力 --&gt; Prometheus基础架构能力 --&gt; CAP理论基础架构能力 --&gt; 领域驱动设计</div><h3 id="微服务架构">微服务架构</h3><div class="mermaid">  graph TD微服务架构  --&gt; 注册中心微服务架构  --&gt; 流量控制微服务架构  --&gt; 分布式事务微服务架构  --&gt; 链路跟踪微服务架构  --&gt; ...</div><h2 id="应用知识">应用知识</h2><h3 id="常用工具">常用工具</h3><div class="mermaid">  graph TD常用工具 --&gt; 排查类常用工具 --&gt; 协作类常用工具 --&gt; 保障类常用工具 --&gt; 系统类</div><h3 id="常用框架">常用框架</h3><div class="mermaid">  graph TD常用框架 --&gt; Spring常用框架 --&gt; Netty常用框架 --&gt; Dubbo常用框架 --&gt; Mybatic常用框架 --&gt; 细分领域常用框架 --&gt; ...</div><h3 id="队列">队列</h3><div class="mermaid">  graph TD常用框架 --&gt; Kafka常用框架 --&gt; ActiveMQ常用框架 --&gt; RabbitMQ常用框架 --&gt; ...</div><h3 id="数据库">数据库</h3><div class="mermaid">  graph TD数据库 --&gt; RMDB数据库 --&gt; NoSql数据库 --&gt; 图数据库数据库 --&gt; 向量数据库数据库 --&gt; ...</div><h3 id="缓存">缓存</h3><div class="mermaid">  graph TD缓存 --&gt; redis缓存 --&gt; ehcache缓存 --&gt; ...</div><h3 id="云平台">云平台</h3><div class="mermaid">  graph TD云平台 --&gt; 阿里云平台 --&gt; 腾讯云平台 --&gt; 华为云平台 --&gt; Ucloud云平台 --&gt; 百度云平台 --&gt; AWS云平台 --&gt; AZure云平台 --&gt; GoogleCloud</div><h2 id="文档能力">文档能力</h2><div class="mermaid">  graph TD文档能力 --&gt; 方案编写文档能力 --&gt; 概要设计文档能力 --&gt; 详细设计</div><p><br><br></p><p>我们抛开JAVA语言本身，将其他语言生态的相关功能置入，同样适用。</p><blockquote><p>当时较懒，对于图中的内容，笔者只更新了两个文章, 不久之后将由其他博客搬至JJ。再有后续的有时间再添加上。</p></blockquote><p><a href="https://juejin.cn/post/7296017029705318419">Jvm原理</a><br><a href="https://juejin.cn/post/7296017029705318419">Jvm调优</a><br>…</p>]]></content>
      
      
      <categories>
          
          <category> Java系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
