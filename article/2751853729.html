<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="PyTorch, TensorFlow,PyTorch入门">
    <meta name="description" content="Tensor的理解
数学中有标量、向量和矩阵的概念，它们的维度分别是0、1、2。其中：


标量可以看成的一个数字，1，标量中元素的位置固定。


向量可以看成是一维表格，向量中元素的位置需要通过其索引确定，表示为



矩阵可以看成是二维">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
	<mate name="baidu-site-verification" content="你的token">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>PyTorch零基础入门-Tensor | 🐘 _山海博客</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="🐘 _山海博客" type="application/atom+xml">
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <span class="logo-span">🐘 _山海博客</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories/Sarah%E5%B0%8F%E7%AB%99" class="waves-effect waves-light">
      
      <i class="fas fa-person-dress" style="zoom: 0.6;"></i>
      
      <span>Sarah小站</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>时间线</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-screencast" style="zoom: 0.6;"></i>
      
      <span>项目</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a target="_blank" rel="noopener" href="https://feicuimeipo.github.io/canvas/">
          
          <i class="fas fa-screencast" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>DataCanvas</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <div class="logo-name">🐘 _山海博客</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories/Sarah%E5%B0%8F%E7%AB%99" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-person-dress"></i>
			
			Sarah小站
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			时间线
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-screencast"></i>
			
			项目
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a target="_blank" rel="noopener" href="https://feicuimeipo.github.io/canvas/ " style="margin-left:75px">
				  
				   <i class="fa fas fa-screencast" style="position: absolute;left:50px" ></i>
			      
		          <span>DataCanvas</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        
    </ul>
</div>


        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/18.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">PyTorch零基础入门-Tensor</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/PyTorch/">
                                <span class="chip bg-color">PyTorch</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/PyTorch%E7%B3%BB%E5%88%97/" class="post-category">
                                PyTorch系列
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-01-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-01-04
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    4.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    19 分
                </div>
                
                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="Tensor的理解">Tensor的理解</h2>
<p>数学中有标量、向量和矩阵的概念，它们的维度分别是0、1、2。其中：</p>
<ul>
<li>
<p><strong>标量</strong>可以看成的一个数字，<strong><code>1</code></strong>，标量中元素的位置固定。</p>
</li>
<li>
<p><strong>向量</strong>可以看成是一维表格，向量中元素的位置需要通过其索引确定，表示为</p>
<p><img src="/article/2751853729/image-20240103024202117.png" alt></p>
</li>
<li>
<p><strong>矩阵</strong>可以看成是二维表格，矩阵中的元素位置需要通过其行号和列号确定，表示为</p>
<p><img src="/article/2751853729/image-20240103024033803.png" alt></p>
</li>
</ul>
<p><strong>张量(Tensor) 可以视为矩阵的扩展，可以用于表示无穷维度的数据</strong></p>
<p>如果我们用标量、向量或矩阵描述一个事物时，该事物最多可用 [H,W]的维度表示。在现实与客观世界中，我们经常会碰到的物体的维度可能会更高维度，很难通过向量或矩阵来描述，这时我们就需要<strong>张量</strong>。也就是说，我们可以通过张量来描述任意维度的物体 <code>H * W * C</code>，其中C为C维的特征图（特征图是深度学习中的一个概念），除外我们还可以用<strong>Tensor</strong>描述更高维度的物体：<code>H * W * C* D</code>，其中D为未知的更高维空间。</p>
<p>总之，张量是对于标量、向量、矩阵之上进行更加泛化的定义。标量可以看成是0阶的张量，向量是1阶段张量，矩阵是2阶的张量，除了0,1,2阶的张量之外，还有<code>3阶、4阶、5阶..n阶</code>的张量。引申到<strong>深度学习</strong>，其 数据输入的维度是不确定的（可以是任意维度），这时就需要采用一个更加广泛的概念去描述这些量，Tensor就可以更便利解决该问题的。</p>
<h2 id="Tensor的基本概念">Tensor的基本概念</h2>
<p><img src="/article/2751853729/image-20240103024323595.png" alt></p>
<p>其中标量是0维的张量，向量是1维的张量，矩阵是2维的张量。</p>
<p>举例：如下所示，左侧为一个长方体某一切面的矩阵（二阶张量），该矩阵包含N*M个元素，其中每一个元素是一个标量，每一张都是一个向量。一个物体有N个切面，将C 个 N * M 拼接到一起，就会得到一个三阶的张量<code>C*N*M</code>用于表示长方体(如下右图)。</p>
<p><img src="/article/2751853729/image-20240103024343658.png" alt></p>
<blockquote>
<p>在用张量描述物体时，我们需要确定这个张量具体是一个什么样的量，用变量或常量来描述。</p>
</blockquote>
<h2 id="Tensor与机器学习的关系">Tensor与机器学习的关系</h2>
<p>完整的机器学习的任务，会涉及到样本、模型等元素（如下所示）。</p>
<p><img src="/article/2751853729/image-20240103024403236.png" alt></p>
<p>对于<strong>样本</strong>（机器学习中用到的数据）我们就可通过Tensor来对其进行描述。<strong>比如一条语音数据</strong>，我们可有能会采用向量来进行描述，该向量就是1阶的张量。此时向量描述的是语音数据被采样后在当前时刻的声音特征，图形化之后可能为波行。而<strong>对于灰度图</strong>，我们通常采用矩阵描述（二阶Tensor），表示成<code>H*W</code>，<strong>彩色图</strong>则会描述成<code>[H * W * C]</code>，为一个三阶Tensor，其中C=3。</p>
<p>而<strong>模型</strong>（模型分为<strong>有参数模型</strong>与<strong>无参数模型</strong>)，有参数模型一般被描述为  <code>Y = WX + b</code>函数，其中X是指样本，W,B是指参数。当W与B未知的情况下为变量，该变量也是通过Tensor来表示，Y为最后的标签，而标签在进行数字化时也会通过Tensor来对其进行描述。</p>
<blockquote>
<p>当样本标签与属性关系描述为<code>y=f(x)</code>，其中x为属性(样本)，f为模型。</p>
</blockquote>
<p>结论：<strong>Tensor可以用来描述机器学习过程中的样本或模型</strong>。</p>
<h2 id="Tensor的操作">Tensor的操作</h2>
<p><img src="/article/2751853729/image-20240102201942723.png" alt></p>
<ul>
<li>类型：Tensor类型</li>
<li>创建：如何创建Tensor</li>
<li>属性：Tensor的属性</li>
<li>运算：Tensor的算术运算</li>
<li>操作：切片、索引、变型等</li>
<li>与<code>numpy</code>互转：Tensor可以与numpy互转</li>
</ul>
<h2 id="Tensor类型">Tensor类型</h2>
<p>张量（Tensor）是Pytorch库中的基本数据类型，在 Pytorch中各种基本数字类型都有其对应的Tensor类型（但在Pytorch中没有内嵌的字符串类型）。</p>
<h3 id="类型列表"><strong>类型列表</strong></h3>
<p>Tensor的每种类型分别有对应CPU和GPU版本。加粗部分为常用类型。Tensor默认的数据类型<code>FloatTensor</code>。</p>
<p>Tensor类型常见的操作见 **<code>Tensor的操作</code>**章节</p>
<table>
<thead>
<tr>
<th>-</th>
<th>数据类型</th>
<th>torch</th>
<th>CPU Tensor</th>
<th>GPU Tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>32-bit float point</strong></td>
<td><strong>32 bit 浮点</strong></td>
<td><strong>torch.float32 or torch.float</strong></td>
<td><strong>torch.FloatTensor</strong></td>
<td><strong>torch.cuda.FloatTensor</strong></td>
</tr>
<tr>
<td><strong>64-bit float point</strong></td>
<td><strong>64 bit 浮点</strong></td>
<td><strong>torch.float64 or torch.double</strong></td>
<td><strong>torch.DoubleTensor</strong></td>
<td><strong>torch.cuda.DoubleTensor</strong></td>
</tr>
<tr>
<td>16-bit float point</td>
<td>16 bit 半精度浮点</td>
<td>torch.float16 or torch.half</td>
<td>torch.HalfTensor</td>
<td>torch.cuda.HalfTensor</td>
</tr>
<tr>
<td><strong>8-bit integer(unsigned)</strong></td>
<td>8 bit 无符号整形(0~255)****</td>
<td><strong>torch.uint8</strong></td>
<td><strong>torch.ByteTensor</strong></td>
<td><strong>torch.cuda.ByteTensor</strong></td>
</tr>
<tr>
<td>8-bit integer(signed)</td>
<td>8 bit 有符号整形(-128~127)</td>
<td>torch.int8</td>
<td>torch.CharTensor</td>
<td>torch.cuda.CharTensor</td>
</tr>
<tr>
<td>16-bit integer(signed)</td>
<td>16 bit 有符号整形</td>
<td>torch.int16 or torch.short</td>
<td>torch.ShortTensor</td>
<td>torch.cuda.ShortTensor</td>
</tr>
<tr>
<td><strong>32-bit integer(signed)</strong></td>
<td><strong>32 bit 有符号整形</strong></td>
<td><strong>torch.int32 or torch.int</strong></td>
<td><strong>torch.IntTensor</strong></td>
<td><strong>torch.cuda.IntTensor</strong></td>
</tr>
<tr>
<td><strong>64-bit integer(signed)</strong></td>
<td><strong>64 bit 有符号整形</strong></td>
<td><strong>torch.int64 or torch.long</strong></td>
<td><strong>torch.LongTensor</strong></td>
<td><strong>torch.cuda LongTensor</strong></td>
</tr>
<tr>
<td>Boolean</td>
<td>布尔</td>
<td>torch.bool</td>
<td>torch.BooleanTensor</td>
<td>torch.cuda.BooleanTensor</td>
</tr>
</tbody>
</table>
<h4 id="List-vs-Nparray">List vs Nparray</h4>
<p>Numpy中的Nparray采用连续地址存储，原生list只能通过寻址方式找到下一种元素；</p>
<p><img src="/article/2751853729/image-20240102223912335.png" alt></p>
<p>这是因为Numpy制定了其存储的数据类型，可以统一分配内存空间，而List中的数据类型是确定的。</p>
<ul>
<li>
<p>Nparray在科学计算方面性能远高于List, 可以省掉许多循环语句</p>
</li>
<li>
<p>Nparray支持并行化运算，底层采用C语言编写，接触了Python解释器的性能限制，所以效率远高于纯Python代码</p>
</li>
</ul>
<h4 id="Numpy-vs-Tensor">Numpy vs Tensor</h4>
<p>Numpy和Tensor相比较，他们的区别如下：</p>
<ul>
<li>Tensor 和 Numpy都是矩阵，区别是前者可以在GPU上运行，后者只能在CPU上。</li>
<li>Tensor可以直接通过print显示数据类型，而Numpy不可以。</li>
</ul>
<blockquote>
<p><strong>在GPU上运行时:</strong>  Tensor内部的数据类型为ndarray，GPU不具有更改元素值的能力，这时Tensor内部元素的数值不可改变</p>
</blockquote>
<h3 id="类型常见操作">类型常见操作</h3>
<ul>
<li>
<p><strong>将普通张量类型转化为GPU张量类型的方法</strong>: <code>普通张量变量名.cuda()</code>, 返回一个GPU张量的引用。</p>
</li>
<li>
<p><strong>Tensor默认的数据类型</strong></p>
<ul>
<li>默认的Tensor是FloatTensor（如果默认类型为GPU tensor，则所有操作都将在GPU上进行）。</li>
<li><strong>设置默认的数据类型</strong>: <strong><code>torch.set_default_tensor_type(类型名)</code></strong></li>
<li>增强学习中使用DoubleTensor的使用会更多</li>
</ul>
</li>
<li>
<p><strong>将张量转换为其他数据类型</strong></p>
<ul>
<li><strong>将张量转换为numpy数组</strong>：<code>张量名.numpy()</code></li>
<li><strong>将只有一个元素的张量转换为标量</strong>：<code>张量名.item()</code>。</li>
</ul>
</li>
<li>
<p><strong>查看张量数据类型的方法</strong></p>
<ul>
<li><strong>type方法</strong>：使用<code>张量名.type()</code>可以查看张量的具体类型。</li>
<li><strong>isinstance</strong>：isinstance是Python的自带函数。用法<code>isinstance(torch.randn(2,3),torch.FloatTensor)</code>，返回布尔值。</li>
</ul>
</li>
<li>
<p><strong>生成元素数据类型指定的张量</strong></p>
<p><strong>浮点型张量</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tensor.FloatTensor(标量/列表/numpy数组)  # 生成元素均为单精度浮点型的张量</span><br><span class="line">tensor.DoubleTensor(标量/列表/numpy数组) # 生成元素均为双精度浮点型的张量</span><br><span class="line">tensor.HalfTensor(标量/列表/numpy数组)   # 生成元素均为半精度浮点型的张量</span><br></pre></td></tr></tbody></table></figure>
<p><strong>整型张量</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tensor.IntTensor(标量/列表/numpy数组)    # 生成元素均为基本整型的张量</span><br><span class="line">tensor.ShortTensor(标量/列表/numpy数组)  # 生成元素均为短整型的张量</span><br><span class="line">tensor.LongTensor(标量/列表/numpy数组)   # 生成元素均为长整型的张量</span><br></pre></td></tr></tbody></table></figure>
<p><strong>布尔型张量</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tensor.BoolTensor(标量/列表/numpy数组)   # 生成元素均为布尔类型的张量</span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
<blockquote>
<p>[<strong>python、numpy、Pytorch中的索引方式</strong>](</p>
</blockquote>
<h2 id="Tensor的创建">Tensor的创建</h2>
<blockquote>
<p>加粗为常用</p>
</blockquote>
<h3 id="创建函数">创建函数</h3>
<table>
<thead>
<tr>
<th>函数</th>
<th>功能</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tensor(*size)</td>
<td>基础构造函数</td>
<td>size: 直接根据形状定义Tensor, 例：torch.tensor(标量|列表)，</td>
</tr>
<tr>
<td>Tensor(data)</td>
<td>类似np.array</td>
<td>data: 使用数据直接初始化,  例：torch.from_numpy(numpy数组)</td>
</tr>
<tr>
<td>*<em>ones(<em>size)</em></em></td>
<td><strong>全1Tensor</strong></td>
<td><strong>常用结构：全部为1的张量</strong></td>
</tr>
<tr>
<td>*<em>zeros(<em>size)</em></em></td>
<td><strong>全0Tensor</strong></td>
<td><strong>常用结构：全部为0的常量</strong></td>
</tr>
<tr>
<td>*<em>eye(<em>size)</em></em></td>
<td><strong>对角线为1，其他为0</strong></td>
<td><strong>常用结构：对角线为1，其他为0</strong></td>
</tr>
<tr>
<td>arange(s,e,step)</td>
<td>从s到e，步长为step</td>
<td>从s到e, 中间的间隔为step，即步长</td>
</tr>
<tr>
<td>linspace(s,e,steps)</td>
<td>从s到e，均匀切分成steps份</td>
<td>从s到e, 均匀切分成steps份</td>
</tr>
<tr>
<td>rand/randn(*size)</td>
<td>均匀/标准分布</td>
<td>size: 根据形状定义Tensor, <strong>值为随机赋值，均匀/标准分布的随机采样</strong></td>
</tr>
<tr>
<td>normal(mean,std)/uniform (from,to)</td>
<td>正态分布/均匀分布</td>
<td>满足<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/514912456">正态分布或均匀分布</a></td>
</tr>
<tr>
<td>randperm(m)</td>
<td>随机排列</td>
<td>对一个序列进行随机排列</td>
</tr>
<tr>
<td>empty(*size)</td>
<td>生成不经过元素初始化的指定形状的张量</td>
<td></td>
</tr>
<tr>
<td>rand_like(tensor)</td>
<td>生成指定填充值的指定形状的张量</td>
<td></td>
</tr>
<tr>
<td>full(*size)</td>
<td>指定值的Tensor</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="编程实例"><strong>编程实例</strong></h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">printTensor</span>(<span class="params">tensor</span>):</span><br><span class="line">    <span class="built_in">print</span>(tensor)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"numel ="</span>, tensor.numel())  <span class="comment"># 输出9</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"dim ="</span>, tensor.dim())  <span class="comment"># 输出2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"type ="</span>, tensor.<span class="built_in">type</span>())  <span class="comment"># 输出2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''常用的Tensor定义'''</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------as_tensor--------------"</span>)</span><br><span class="line">shape = [[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>]]</span><br><span class="line">tensor = torch.as_tensor(shape)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------Tensor(* Size)--------------"</span>)</span><br><span class="line">shape = [(<span class="number">2</span>, <span class="number">3</span>), (<span class="number">4</span>, <span class="number">5</span>), [<span class="number">6</span>, <span class="number">7</span>]]</span><br><span class="line">tensor1 = torch.Tensor(shape)</span><br><span class="line">printTensor(tensor1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------numpy.array()--------------"</span>)</span><br><span class="line">data_array = np.array([[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line">tensor = torch.tensor(data_array)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个3行3列的张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------torch.ones--------------"</span>)</span><br><span class="line">tensor = torch.ones((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------torch.zeros--------------"</span>)</span><br><span class="line">tensor = torch.zeros(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------torch.zeros_like--------------"</span>)</span><br><span class="line">tensorlike = torch.zeros_like(tensor1)</span><br><span class="line">printTensor(tensorlike)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------------torch.randn--------------"</span>)</span><br><span class="line">tensor = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="string">'''正态分布'''</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.normal mean为均值，std为标准差-1--------------"</span>)</span><br><span class="line"><span class="comment"># std=5组不同的正诚分布，5组都是随机的标准差和 mean =0</span></span><br><span class="line">tensor = torch.normal(mean=<span class="number">0.0</span>, std=torch.rand(<span class="number">5</span>))</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.normal mean为均值，std为标准差-2--------------"</span>)</span><br><span class="line"><span class="comment"># std=5组不同的正诚分布，5组都是随机的标准差和随机的mean</span></span><br><span class="line">tensor = torch.normal(mean=torch.rand(<span class="number">5</span>), std=torch.rand(<span class="number">5</span>))</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.uniform_--------------"</span>)</span><br><span class="line">tensor = torch.Tensor(<span class="number">4</span>, <span class="number">2</span>).uniform_()</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''定义一个序列'''</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.arange--------------"</span>)</span><br><span class="line"><span class="comment"># 定义一个序列, 步长为2， 最后10不包含在序列中</span></span><br><span class="line">tensor = torch.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">u"--------------torch.linspace--------------"</span>)</span><br><span class="line"><span class="comment"># 等间节切分,5为个数，11为范围，0为起始值</span></span><br><span class="line">tensor = torch.linspace(<span class="number">0</span>, <span class="number">11</span>, <span class="number">5</span>)</span><br><span class="line">printTensor(tensor)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="Tensor的属性">Tensor的属性</h2>
<ul>
<li>每一个Tensor有torch.dtype、torch.device、torch.layout三种属性</li>
<li>torch.device 标识了torch.Tensor对象在创建之后所存储在的设备名称</li>
<li>torch.layout表示torch.Tensor内存布局的对象</li>
</ul>
<blockquote>
<p>torch.dtype： 在使用tensor函数创建tensor张量对象时还可以使用dtype参数指定数据类型</p>
<p>torch.device:  张量所创建的数据，到底应该存储在哪个设备上，CPU或GPU, GPU是通过CUDA来表示，多个则用cuda:0, cuda:1，以此类推</p>
<p>torch.layout:  张量的排布方式，对应到内存中连续的区别。稠密或稀疏的方式。</p>
</blockquote>
<h3 id="稠密的张量">稠密的张量</h3>
<p><strong>稠密的张量定义方法</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># GPU</span></span><br><span class="line"><span class="comment"># GPU则代码改为： dev = torch.device("cuda:0")</span></span><br><span class="line">dev = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], dtype=torch.float32,device=dev)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="稀疏的张量"><strong>稀疏的张量</strong></h3>
<p><strong>稀疏或低秩</strong>是机器学习中两个很重要的概念，描述了当前数据是否满足某种性质</p>
<ul>
<li>
<p>稀疏表达了当前数据中，非0元素的个数，非0元素的个数越少，说明越稀疏。如果全部为0，则说明最稀疏。</p>
</li>
<li>
<p>低秩描述了数据本身的关联性，也是线性代码中的一个概念。<strong>秩</strong>从线性相关的角度来看，主要是描述了当前矩阵中的向量间线性可表示的关系。</p>
</li>
</ul>
<p><strong>稀疏的张量在机器学习中的优势</strong></p>
<ul>
<li>从模型角度：能够使<strong>模型</strong>变的非常简单。对于有参数的模型，如果参数中0的个数非常多，意味着可以对模型进行简化。即参数为0的项(item) 是可以减掉的，因为0乘以任何数都等于0。这样参数个数变少，意味着模型变的更简单，对于参数稀疏化的约束在机器学习中是一个非常重要的性质。这是我们从机器学习模型的角度上介绍稀疏的意义。</li>
<li>从数据角度，通过对数据进行稀疏化的表示，可以减少数据在内存中的开销。假设存在一个100*100的矩阵，哪果用稠密方式表示数据，则需要100**100单位的空间，而如果用稀疏张量表示，我们只要记住非0元素的坐标即可。</li>
</ul>
<p><strong><code>torch.sparse_coo_tensor</code></strong></p>
<p>PyTorch中用<code>torch.sparse_coo_tensor</code> 表示稀疏矩阵。使用<code>torch.sparse_coo_tenso</code>r可以方便地将稀疏矩阵转换为PyTorch张量，并进行各种操作。同时，由于只存储了非零元素的位置和值，因此可以节省大量的内存空间。</p>
<blockquote>
<p>名称是 'coo’代表非零元素的坐标。即<strong>coo类型</strong>:  coo类型表示了非零元素的坐标形式</p>
</blockquote>
<p><strong><code>torch.sparse_coo_tensor</code>参数说明</strong>：</p>
<ul>
<li>
<p>indices:  一个二维的LongTensor,  表示<strong>非零元素在原矩阵中的位置</strong>，其形状为(N, 2),其中N为非零元素个数。</p>
</li>
<li>
<p>values: 一个一维的Tensor, 表示<strong>非零元素的值</strong>，其形状为(N,)。</p>
</li>
<li>
<p>size: 一个元组，表示<strong>输出张量的形状</strong>，例如(M, N)。</p>
</li>
</ul>
<p><strong><code>torch.sparse_coo_tensor</code>参数解说</strong>：</p>
<ul>
<li>indices: 表示非零元素在原矩阵中的位置，即哪些位置是非零的。它的形状为(N, 2),  其中<strong>N为非零元素个数。每个元素包含两个值，分别表示该非零元素在行和列上的位置</strong>。</li>
<li>values： 表示非零元素的值，即<strong>这些位置上的数值</strong>。它的形状为(N,)</li>
<li>size： 表示输出张量的形状，即输出张量的行数和列数。例如，如果输入矩阵是一个4x5的矩阵，但是只有第2行和第4行、第3列和第5列上的元素是非零的，那么输出张量的形状就是(2, 2)。</li>
</ul>
<p><strong><code>torch.sparse_coo_tensor</code>用法</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dev = torch.device(<span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义长度分别为3个长度的坐标: [0,2] [1,0] [1,2],</span></span><br><span class="line"><span class="comment"># indices = torch.tensor([[0, 1, 2], [0, 1, 2]]) 会保存数据落地对角线上</span></span><br><span class="line">indices = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="comment"># 以上3组从坐标对应的三个非0元素 3,4,5</span></span><br><span class="line">values = torch.tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 原张量的形状是一个2,4的tensor, 如果用稠密方式打印，打看到一个2,4的变量</span></span><br><span class="line">x = torch.sparse_coo_tensor(indices, values, [<span class="number">3</span>, <span class="number">3</span>], device=dev, dtype=torch.float32)</span><br><span class="line">x_to_dense = x.to_dense()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------sparse_coo_tensor------------"</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"--------x_to_dense------------"</span>)</span><br><span class="line"><span class="built_in">print</span>(x_to_dense)</span><br></pre></td></tr></tbody></table></figure>
<p>控制台输出</p>
<p><img src="/article/2751853729/image-20240103020805785.png" alt></p>
<p><strong>例：将数据落地对角线上</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">indices = torch.tensor([[0, 1, 2], [0, 1, 2]]) </span><br><span class="line">values = torch.tensor([1,2,3], dtype=torch.float32)</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><strong>解释</strong>：</p>
<p>i的坐标 （0,0）（1,1）（2,2），第0行代表x，第1行代表y</p>
<p>矩阵的坐标：   [（0,0）  （0,1） （0,2）</p>
<p>​                           （1,0）  （1,1） （1,2）</p>
<p>​                           （2,0）  （2,1） （2,2） ]</p>
<p><strong>数据正好在对角线</strong></p>
</blockquote>
<h2 id="Tensor的算术运算">Tensor的算术运算</h2>
<p><strong>四则运算</strong></p>
<ul>
<li>加减乘除</li>
<li>矩阵运算</li>
</ul>
<p><strong>其他运算</strong></p>
<ul>
<li>torch.pow - 幂运算</li>
<li>torch.exp(input, out=None) - e指数，注意只支持浮点型</li>
<li>torch.sqrt(input, out=None) - 开方</li>
<li>torch.log(input, out=None) - 对数运算，以e为底</li>
<li>ceil/round/floor/trunc - 取整/四舍五入/下取整/只保留整数部分 - 如<code>torch.ceil(input, out=None)</code></li>
<li>clamp(input, min, max) - 超过min和max部分截断 - <code>torch.clamp(input, min, max, out=None)</code></li>
<li>torch.abs(input, out=None)- 求绝对值</li>
<li>…</li>
</ul>
<h3 id="加减法运算"><strong>加减法运算</strong></h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">c = a + b  <span class="comment"># a - b ,</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line">c = torch.add(a, b)  <span class="comment"># 减法sub</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(a.add(b)) <span class="comment"># 减法sub，</span></span><br><span class="line"><span class="comment"># a.add_(b)  # sub_(...)， 带下划线的方式，运算后将结果同时赋给a。加减乘除等运算中含有下划线"_"的规则都一样。</span></span><br><span class="line"><span class="built_in">print</span>(a) </span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>减法：- / …sub() / …sub_()<br>
乘法: * / <code> c = torch.mul(a,b)</code>  /<code>a.mul(b)</code>  /<code>a.mul_(b)</code></p>
</blockquote>
<h3 id="哈达玛积">哈达玛积</h3>
<p><strong>哈达玛积(element wise，对应元素相乘) - mul乘法</strong> 如果一个tensor是shape是<code>2 * 2</code> 的话，则另外一个tensor也是<code>(2 * 2)</code>,  这样保证所有元素都保证每个元素都可以被相乘。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">c = a * b</span><br><span class="line">c = torch.mul(a, b) <span class="comment"># a和b的shape要一样的</span></span><br><span class="line">a.mul(b)</span><br><span class="line">a.mul_(b) <span class="comment"># 同时将结果赋给a</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="矩阵运算">矩阵运算</h3>
<h4 id="二维矩阵的乘法运算">二维矩阵的乘法运算</h4>
<p>**二维矩阵的乘法运算 **包括<code>torch.mm(),torch.matmul(),@</code></p>
<p>规则<code> a * b</code> 时，即两个矩阵相乘，<code>m * n</code>,  <code>n * p</code>, 一定要保证 两个矩阵的’<strong>n</strong>’是相同的。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">b = torch.ones(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.mm(a,b))</span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b))</span><br><span class="line"><span class="built_in">print</span>(a @ b)</span><br><span class="line"><span class="built_in">print</span>(a.matmul(b))</span><br><span class="line"><span class="built_in">print</span>(a.mm(b))</span><br></pre></td></tr></tbody></table></figure>
<h4 id="高维矩阵的乘法运算"><strong>高维矩阵的乘法运算</strong></h4>
<p>对于高维的<code>Tensor(dim&gt;2)</code>，假如矩阵的<code>size=(a1,a2,m,n)</code> ，我们要保证除最后两维<code>m,n</code>之外的前几维的值保持一致, 最后两维的规则同二维矩阵，即n相同 。就像矩阵的索引一样并且运算操只有``torch.matmul()`</p>
<blockquote>
<p>同样是除了矩阵内的数值之外，要保证维度的每个元素都可以被计算。<br>
<code>mm</code>()与<code>matmul()</code>不存在下划线类的计算.。</p>
</blockquote>
<p>如下所示：<code>n</code>相同，<code>a,b</code>相同。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.ones(a, b, m, n)</span><br><span class="line">b = torch.ones(a, b, n, p)</span><br></pre></td></tr></tbody></table></figure>
<p>举例：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.ones(1, 2, 4, 5)</span><br><span class="line">b = torch.ones(1, 2, 5, 3)</span><br><span class="line">print(a.matmul(b))</span><br><span class="line">print(torch.matmul(a, b))</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/185249529">矩阵的乘除与逆运算</a></p>
</blockquote>
<h3 id="幂运算"><strong>幂运算</strong></h3>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.full((2, 3), fill_value=2)</span><br><span class="line">print(torch.pow(a, 2))</span><br><span class="line">print(a.pow(2))</span><br><span class="line">print(a ** 2)</span><br><span class="line">print(a.pow_(2))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="指数运算"><strong>指数运算</strong></h3>
<p><strong>指数函数：<strong>函数y=a^x(a&gt;0且a≠1) 叫做</strong>指数函数</strong>，a是常数，x是自变量，定义域为R，值域为（0，+∞）。要求：a^x前的系数必须是数1，自变量x必须在指数的位置上，且不能是x的其他表达式，否则就不是指数函数。</p>
<img src="/article/2751853729/image-20240103195933889.png" alt="image-20240103195933889" style="zoom:67%;">
<ul>
<li>a&gt;1时，则指数函数单调递增；若0&lt;a&lt;1，则为单调递减的。</li>
<li>对于a不大于0的情况，函数的定义域不连续，不考虑; a等于0函数无意义一般也不考虑。</li>
<li>指数函数恒过（0，1）点，即水平直线y=1是从递减到递增的一个过渡位置。</li>
<li>指数函数是非奇非偶函数。</li>
<li>…</li>
</ul>
<p><strong>指数函数应用到自然常数e上写为exp(x)，现常写为e^x</strong>（表示为x=lny），其图像是单调递增，n∈R，y&gt;0，与y轴相交于（0,1）点。，图像位于X轴上方，第二象限无限接近X轴。</p>
<h4 id="e的n次方">e的n次方</h4>
<p>$$<br>
y = e^n \ n∈R，y&gt;0，与y轴相交于(0,1)点<br>
$$</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.full((2, 3), fill_value=2)</span><br><span class="line">print(torch.exp(a))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="开方运算"><strong>开方运算</strong></h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.full((<span class="number">2</span>, <span class="number">3</span>), fill_value=<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a.sqrt())</span><br><span class="line"></span><br><span class="line"><span class="comment"># ----返回值-----</span></span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="对数运算"><strong>对数运算</strong></h3>
<p>对数源于指数，是指数函数反函数 因为：N = a<sup>x&nbsp;</sup>  所以：x = log(a<sup>N</sup>)</p>
<p>如果 N=a^x（a&gt;0,a≠1），即<em>a</em>的<em>x</em>次方等于<em>N</em>（<em>a</em>&gt;0，且<em>a</em>≠1），那么数<em>x</em>叫做以<em>a</em>为底<em>N</em>的对数（logarithm），记作：x=log（a<sup>N</sup>）　其中，<em>a</em>叫做对数的底数，<em>N</em>叫做真数，<em>x</em>叫做 “以<em>a</em>为底<em>N</em>的对数”。</p>
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzIwNzA1NDkzMg==&amp;mid=2651391217&amp;idx=1&amp;sn=ca5a92928be3100b9f8565931b3c386f">对数运算规则</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39852647/article/details/110869824">数据增长率怎么算</a> -  对数是用来衡量增长率的: 对数函数中，x是自变量，y是因变量，当x大于1时，对数函数是增函数，所以取对数就是增长率。</li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">a = torch.full((2, 3), fill_value=4)</span><br><span class="line">print(torch.log2(a))</span><br><span class="line">print(torch.log10(a))</span><br><span class="line">print(torch.log(a))</span><br></pre></td></tr></tbody></table></figure>
<h2 id="Tensor的更多操作">Tensor的更多操作</h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/a171232886/article/details/121458916"><strong>python、numpy、Pytorch中的索引方式</strong></a></p>
</blockquote>
<h3 id="维度调整">维度调整</h3>
<ul>
<li>
<p>查看维度：<code>torch.shape</code></p>
</li>
<li>
<p>变换维度：<code>torch.reshape([d0,d1,d2])</code></p>
</li>
<li>
<p><code>unsqueeze</code>和<code>squeeze</code></p>
<ul>
<li>
<p><code>b=a.squeeze(a)</code> 去掉维度值为1的维度</p>
</li>
<li>
<p><code>b=a.unsqueeze(n)</code> 增加一个维度在shape的第n个</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">c = torch.rand(2, 3)</span><br><span class="line">c = c.unsqueeze(1)</span><br><span class="line">print(c.shape)</span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="两个tensor合并">两个tensor合并</h3>
<ul>
<li><code>torch.cat(tensors, dim=0, out=None)</code></li>
<li><code>torch.stack(tensors)</code>合并时候新建一个维度</li>
</ul>
<h3 id="与numpy互换">与numpy互换</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/352877584">图解PyTorch中的torch.gather函数 - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/lilai619/article/details/123083492">pytorch和numpy的互转</a> ，有以下几种方式：
<ul>
<li><code>torch.gather</code>对应的<code>numpy</code>操作：</li>
<li><code>torch.view</code>对应的<code>numpy</code>操作</li>
<li><code>torch.argmax</code>对应的<code>numpy</code>操作</li>
<li><code>torch.max</code>对应的<code>numpy</code>操作</li>
</ul>
</li>
</ul>
<p><strong>参考</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hanmo22357/article/details/129524092">https://blog.csdn.net/hanmo22357/article/details/129524092</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a171232886/article/details/123900387">https://blog.csdn.net/a171232886/article/details/123900387</a></li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">_山海</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://blog.nianxi.cc/article/2751853729.html">https://blog.nianxi.cc/article/2751853729.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">_山海</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/PyTorch/">
                                    <span class="chip bg-color">PyTorch</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'mHuBCqi55c2fbVfnclSExp3n-gzGzoHsz',
        appKey: 'LMInvIagUHK1LeTWJKF4x2Yw',
        serverURLs: 'https://mhubcqi5.lc-cn-n1-shared.com',
        notify: 'true' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: 'just go go'
    });
</script>

<!--酷Q推送-->


    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/article/2751853729.html">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/18.jpg" class="responsive-img" alt="PyTorch零基础入门-Tensor">
                        
                        <span class="card-title">PyTorch零基础入门-Tensor</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/PyTorch%E7%B3%BB%E5%88%97/" class="post-category">
                                    PyTorch系列
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/PyTorch/">
                        <span class="chip bg-color">PyTorch</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/article/2348287028.html">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/9.jpg" class="responsive-img" alt="PyTorch零基础入门-机器学习基础">
                        
                        <span class="card-title">PyTorch零基础入门-机器学习基础</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-01-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/PyTorch%E7%B3%BB%E5%88%97/" class="post-category">
                                    PyTorch系列
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">机器学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE' || selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 🐘 _山海博客<br />'
            + '文章作者: _山海<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    

    <div class="container row center-align" style="margin-bottom: 0px !important;">
	
        <div class="col s12 m8 l8 copy-right"> 			
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
			
            <a href="/about" target="_blank">nianxi.cc</a>
			
           <!-- |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
			-->
            
			
			
               &nbsp;&nbsp;&nbsp;&nbsp; <span id="icp"><img src="/medias/icp.png"   style="vertical-align: text-bottom;"/>
                <a href="https://beian.miit.gov.cn/" target="_blank">&nbsp;京ICP备2023026358号-1</a>
            </span>
            
			
            <br> 
			
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11.1k</span>
            
            
			
				<!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?98e3ae2de563b1e52a1ec856b05a125f";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

			
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;&nbsp;&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>
            <!-- 运行天数提醒. -->
            
            <br>

        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/feicuimeipo/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:xlnian@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/nxkf" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/nxkf" data-position="top" data-delay="50">
        <i class="fab">知</i>
    </a>



    <a href="https://juejin.cn/user/272334612598664" class="tooltipped" target="_blank" data-tooltip="关注我的掘金" data-position="top" data-delay="50">
	      <i class="fas">掘</i>
    </a>



    <a href="https://www.jianshu.com/u/dea235f76a8a" class="tooltipped" target="_blank" data-tooltip="关注我的简书" data-position="top" data-delay="50">
        <i class="fab">简</i>
    </a>


    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>



    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    
        <!-- <script src='https://unpkg.com/mermaid@latest/dist/mermaid.min.js'></script> -->
        <script src='/libs/mermaid/mermaid.min.js'></script>
        <script>
          if (window.mermaid) {
            mermaid.initialize({theme: 'forest'});
          }
        </script>
    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?98e3ae2de563b1e52a1ec856b05a125f";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
